[{"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\agents\\interview-agent.ts":"1","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\agent\\route.ts":"2","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\extract-text\\route.ts":"3","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\livekit\\route.ts":"4","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\openai-gpt\\route.ts":"5","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\speak\\route.ts":"6","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\speech-to-text\\route.ts":"7","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\AudioChat.tsx":"8","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\BackgroundAnimation.tsx":"9","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\Chat.tsx":"10","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\ResumeUploader.tsx":"11","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\SoundEffects.ts":"12","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\layout.tsx":"13","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\page.tsx":"14","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\pdfjs-dist-shims.d.ts":"15","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\react-chat-plugin.d.ts":"16"},{"size":2389,"mtime":1743202398936,"results":"17","hashOfConfig":"18"},{"size":3451,"mtime":1743202398942,"results":"19","hashOfConfig":"18"},{"size":2907,"mtime":1743218500806,"results":"20","hashOfConfig":"18"},{"size":2254,"mtime":1743202398946,"results":"21","hashOfConfig":"18"},{"size":3096,"mtime":1743478109227,"results":"22","hashOfConfig":"18"},{"size":2245,"mtime":1743202398946,"results":"23","hashOfConfig":"18"},{"size":1079,"mtime":1743202398960,"results":"24","hashOfConfig":"18"},{"size":13662,"mtime":1743477696891,"results":"25","hashOfConfig":"18"},{"size":10695,"mtime":1743218500806,"results":"26","hashOfConfig":"18"},{"size":1724,"mtime":1743494949402,"results":"27","hashOfConfig":"18"},{"size":3649,"mtime":1743477849347,"results":"28","hashOfConfig":"18"},{"size":670,"mtime":1743233822829,"results":"29","hashOfConfig":"18"},{"size":900,"mtime":1743202398978,"results":"30","hashOfConfig":"18"},{"size":1620,"mtime":1743477721044,"results":"31","hashOfConfig":"18"},{"size":188,"mtime":1743494949214,"results":"32","hashOfConfig":"18"},{"size":550,"mtime":1743494949295,"results":"33","hashOfConfig":"18"},{"filePath":"34","messages":"35","suppressedMessages":"36","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},"er8919",{"filePath":"37","messages":"38","suppressedMessages":"39","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"40","messages":"41","suppressedMessages":"42","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"43","messages":"44","suppressedMessages":"45","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"46","messages":"47","suppressedMessages":"48","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"49","messages":"50","suppressedMessages":"51","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"52","messages":"53","suppressedMessages":"54","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"55","messages":"56","suppressedMessages":"57","errorCount":0,"fatalErrorCount":0,"warningCount":2,"fixableErrorCount":0,"fixableWarningCount":0,"source":"58"},{"filePath":"59","messages":"60","suppressedMessages":"61","errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"62"},{"filePath":"63","messages":"64","suppressedMessages":"65","errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"66"},{"filePath":"67","messages":"68","suppressedMessages":"69","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"70","messages":"71","suppressedMessages":"72","errorCount":0,"fatalErrorCount":0,"warningCount":1,"fixableErrorCount":0,"fixableWarningCount":0,"source":"73"},{"filePath":"74","messages":"75","suppressedMessages":"76","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"77","messages":"78","suppressedMessages":"79","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"80","messages":"81","suppressedMessages":"82","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},{"filePath":"83","messages":"84","suppressedMessages":"85","errorCount":0,"fatalErrorCount":0,"warningCount":0,"fixableErrorCount":0,"fixableWarningCount":0},"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\agents\\interview-agent.ts",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\agent\\route.ts",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\extract-text\\route.ts",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\livekit\\route.ts",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\openai-gpt\\route.ts",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\speak\\route.ts",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\api\\speech-to-text\\route.ts",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\AudioChat.tsx",["86","87"],[],"'use client';\r\n\r\nimport { useEffect, useState, useRef } from 'react';\r\nimport Image from 'next/image';\r\nimport {\r\n  LiveKitRoom,\r\n  RoomAudioRenderer,\r\n} from '@livekit/components-react';\r\nimport '@livekit/components-styles';\r\nimport { useChat } from 'ai/react';\r\nimport BackgroundAnimation from './BackgroundAnimation';\r\nimport { playTransformersSound, playGameEndSound } from './SoundEffects';\r\n\r\ntype AudioChatProps = {\r\n  initialText?: string;\r\n};\r\n\r\nconst TypeWriter = ({ text }: { text: string }) => {\r\n  const [displayedText, setDisplayedText] = useState(\"\");\r\n  const intervalRef = useRef<number | null>(null);\r\n\r\n  useEffect(() => {\r\n    setDisplayedText(\"\"); // Reset text when prop changes\r\n    let currentText = \"\";\r\n    let currentIndex = 0;\r\n\r\n    const streamText = () => {\r\n      if (currentIndex < text.length) {\r\n        currentText += text[currentIndex];\r\n        setDisplayedText(currentText);\r\n        currentIndex++;\r\n      } else {\r\n        if (intervalRef.current) {\r\n          window.clearInterval(intervalRef.current);\r\n        }\r\n      }\r\n    };\r\n\r\n    intervalRef.current = window.setInterval(streamText, 50);\r\n\r\n    return () => {\r\n      if (intervalRef.current) {\r\n        window.clearInterval(intervalRef.current);\r\n      }\r\n    };\r\n  }, [text]);\r\n\r\n  return (\r\n    <h2 className=\"typewriter-text\">\r\n      {displayedText}\r\n    </h2>\r\n  );\r\n};\r\n\r\nconst AudioChat: React.FC<AudioChatProps> = ({ initialText }) => {\r\n  const [isConnected, setIsConnected] = useState(false);\r\n  const [error, setError] = useState<string | null>(null);\r\n  const [token, setToken] = useState<string | null>(null);\r\n  const [roomName, setRoomName] = useState<string | null>(null);\r\n  const [wsUrl, setWsUrl] = useState<string | null>(null);\r\n  const [isRecording, setIsRecording] = useState(false);\r\n  const [isProcessing, setIsProcessing] = useState(false);\r\n  const [messages, setMessages] = useState<Array<{ role: string; content: string }>>([]);\r\n  const [isUIReady, setIsUIReady] = useState(false);\r\n  const [isInterviewComplete, setIsInterviewComplete] = useState(false);\r\n\r\n  // Use the useChat hook for better message handling\r\n  const { append, messages: chatMessages } = useChat({\r\n    api: '/api/openai-gpt',\r\n    onFinish: (message) => {\r\n      // Only speak the complete message when it's fully received\r\n      if (message.role === 'assistant') {\r\n        speakText(message.content).catch(console.error);\r\n\r\n        // Check if this is the final message (contains the goodbye message)\r\n        // Look for the key phrases that indicate the interview is complete\r\n        if (message.content.includes('Thank you for your time') &&\r\n          message.content.includes('have a great day')\r\n        ) {\r\n          setIsInterviewComplete(true);\r\n        }\r\n      }\r\n    }\r\n  });\r\n\r\n  // MediaRecorder setup\r\n  const mediaRecorder = useRef<MediaRecorder | null>(null);\r\n  const audioChunks = useRef<Blob[]>([]);\r\n\r\n  // Add ref for messages container\r\n  const messagesEndRef = useRef<HTMLDivElement>(null);\r\n\r\n  // Scroll to bottom when messages change\r\n  useEffect(() => {\r\n    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });\r\n  }, [messages]);\r\n\r\n  // Play transformers sound when loading\r\n  useEffect(() => {\r\n    if (!isConnected || !token || !roomName || !wsUrl) {\r\n      playTransformersSound();\r\n    }\r\n  }, [isConnected, token, roomName, wsUrl]);\r\n\r\n  // Play game-end sound when interview is complete\r\n  useEffect(() => {\r\n    if (isInterviewComplete) {\r\n      playGameEndSound();\r\n    }\r\n  }, [isInterviewComplete]);\r\n\r\n  // Function to send text to speech\r\n  const speakText = async (text: string) => {\r\n    try {\r\n      // Create a new SpeechSynthesisUtterance\r\n      const utterance = new SpeechSynthesisUtterance(text);\r\n\r\n      // Set some properties for better speech\r\n      utterance.rate = 1.0;  // Speed of speech\r\n      utterance.pitch = 1.0; // Pitch of voice\r\n      utterance.volume = 1.0; // Volume\r\n\r\n      // Return a promise that resolves when the speech is complete\r\n      return new Promise<void>((resolve, reject) => {\r\n        utterance.onend = () => {\r\n          console.log('Finished speaking chunk');\r\n          console.log('Text-to-speech completed successfully');\r\n          resolve();\r\n        };\r\n\r\n        utterance.onerror = (error) => {\r\n          console.error('Error in text-to-speech:', error);\r\n          reject(error);\r\n        };\r\n\r\n        // Start speaking\r\n        console.log('Starting to speak:', text);\r\n        window.speechSynthesis.speak(utterance);\r\n      });\r\n    } catch (error) {\r\n      console.error('Error in text-to-speech:', error);\r\n    }\r\n  };\r\n\r\n  // Function to send message to OpenAI and speak\r\n  const sendMessageToOpenAI = async (messageContent: string, role: 'system' | 'user' = 'user') => {\r\n    try {\r\n      // If it's a system message (like the initial greeting), speak it directly\r\n      if (role === 'system') {\r\n        setMessages(prev => [...prev, { role, content: messageContent }]);\r\n        await speakText(messageContent);\r\n        return;\r\n      }\r\n\r\n      // For user messages, use the useChat hook's append function\r\n      await append({\r\n        content: messageContent,\r\n        role: 'user'\r\n      });\r\n    } catch (error) {\r\n      console.error('Error in sendMessageToOpenAI:', error);\r\n      setError('Failed to get AI response');\r\n    }\r\n  };\r\n\r\n  // Update messages when chat messages change\r\n  useEffect(() => {\r\n    if (chatMessages.length > 0) {\r\n      setMessages(chatMessages);\r\n\r\n      // We no longer trigger speech here since we're using onFinish\r\n      // The UI will still update progressively with the stream\r\n    }\r\n  }, [chatMessages]);\r\n\r\n  useEffect(() => {\r\n    const setupRoom = async () => {\r\n      try {\r\n        console.log('Setting up room...');\r\n        const generatedRoomName = `interview-${Math.random().toString(36).substring(2, 8)}`;\r\n        setRoomName(generatedRoomName);\r\n\r\n        const response = await fetch('/api/agent', {\r\n          method: 'POST',\r\n          headers: {\r\n            'Content-Type': 'application/json',\r\n          },\r\n          body: JSON.stringify({\r\n            roomName: generatedRoomName,\r\n            participantName: 'User',\r\n          }),\r\n        });\r\n\r\n        if (!response.ok) {\r\n          throw new Error('Failed to get token');\r\n        }\r\n\r\n        const data = await response.json();\r\n        console.log('API Response:', data);\r\n\r\n        if (!data.token || typeof data.token !== 'string') {\r\n          throw new Error('Invalid token received from server');\r\n        }\r\n\r\n        if (!data.wsUrl) {\r\n          throw new Error('WebSocket URL not received from server');\r\n        }\r\n\r\n        setToken(data.token);\r\n        setWsUrl(data.wsUrl);\r\n        setIsConnected(true);\r\n        setIsUIReady(true); // Mark UI as ready after connection is established\r\n\r\n        // Send initial message after UI is ready\r\n        const initialMessage = initialText ?? 'Hello, I am Bob the Interviewer. How can I help you?';\r\n        await sendMessageToOpenAI(initialMessage, 'system');\r\n      } catch (err) {\r\n        console.error('Error setting up room:', err);\r\n        setError(err instanceof Error ? err.message : 'Failed to connect to interview room');\r\n      }\r\n    };\r\n\r\n    setupRoom();\r\n  }, [initialText]);\r\n\r\n  // Initialize MediaRecorder\r\n  useEffect(() => {\r\n    const initMediaRecorder = async () => {\r\n      try {\r\n        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\r\n        mediaRecorder.current = new MediaRecorder(stream);\r\n\r\n        mediaRecorder.current.ondataavailable = (event) => {\r\n          if (event.data.size > 0) {\r\n            audioChunks.current.push(event.data);\r\n          }\r\n        };\r\n\r\n        mediaRecorder.current.onstop = async () => {\r\n          setIsProcessing(true);\r\n          try {\r\n            const audioBlob = new Blob(audioChunks.current, { type: 'audio/wav' });\r\n            audioChunks.current = [];\r\n\r\n            // Create form data with the audio file\r\n            const formData = new FormData();\r\n            formData.append('audio', audioBlob, 'recording.wav');\r\n\r\n            // Send audio to speech-to-text endpoint\r\n            const transcriptionResponse = await fetch('/api/speech-to-text', {\r\n              method: 'POST',\r\n              body: formData,\r\n            });\r\n\r\n            if (!transcriptionResponse.ok) {\r\n              throw new Error('Failed to transcribe audio');\r\n            }\r\n\r\n            const { text: transcribedText } = await transcriptionResponse.json();\r\n            console.log('Transcribed text:', transcribedText);\r\n\r\n            // Send transcribed text to OpenAI\r\n            await sendMessageToOpenAI(transcribedText);\r\n          } catch (error) {\r\n            console.error('Error processing audio:', error);\r\n            setError('Failed to process audio');\r\n          } finally {\r\n            setIsProcessing(false);\r\n          }\r\n        };\r\n      } catch (err) {\r\n        console.error('Error initializing media recorder:', err);\r\n        setError('Failed to access microphone');\r\n      }\r\n    };\r\n\r\n    if (isConnected) {\r\n      initMediaRecorder();\r\n    }\r\n  }, [isConnected]);\r\n\r\n  const toggleRecording = () => {\r\n    if (!mediaRecorder.current) {\r\n      console.error('MediaRecorder not initialized');\r\n      return;\r\n    }\r\n\r\n    if (isRecording) {\r\n      mediaRecorder.current.stop();\r\n    } else {\r\n      audioChunks.current = [];\r\n      mediaRecorder.current.start();\r\n    }\r\n    setIsRecording(!isRecording);\r\n  };\r\n\r\n  if (error) {\r\n    return (\r\n      <div className=\"error-container\">\r\n        <p>Error: {error}</p>\r\n        <button onClick={() => window.location.reload()}>Retry Connection</button>\r\n      </div>\r\n    );\r\n  }\r\n\r\n  if (!isConnected || !token || !roomName || !wsUrl) {\r\n    return (\r\n      <div className=\"loading-container\">\r\n        <div className=\"loading-message\">\r\n          <div className=\"robot-image-container\">\r\n            <Image\r\n              src=\"/MegaRobotInterviewer.png\"\r\n              alt=\"Robot Interviewer\"\r\n              width={480}\r\n              height={480}\r\n              priority\r\n            />\r\n          </div>\r\n          <div className=\"loading-text\">\r\n            <TypeWriter text=\"Hello! I'm your interviewer today. I'll be reviewing your resume and asking you some questions.\" />\r\n            <h3>Connecting to interview room...</h3>\r\n            <div className=\"loading-spinner\"></div>\r\n          </div>\r\n        </div>\r\n      </div>\r\n    );\r\n  }\r\n\r\n  return (\r\n    <>\r\n      <BackgroundAnimation />\r\n      <div className=\"audio-chat-container\">\r\n        <LiveKitRoom\r\n          token={token}\r\n          serverUrl={wsUrl}\r\n          connect={true}\r\n          onConnected={() => {\r\n            console.log('Connected to LiveKit room:', roomName);\r\n          }}\r\n          onDisconnected={() => {\r\n            console.log('Disconnected from LiveKit room:', roomName);\r\n          }}\r\n          onError={(error) => {\r\n            console.error('LiveKit room error:', error);\r\n            setError(error.message);\r\n          }}\r\n        >\r\n          <div className=\"chat-layout\">\r\n            <div className=\"chat-header\">\r\n              <h2>Interview with Bob</h2>\r\n              <div className=\"status-indicator\">\r\n                <span className={`status-dot ${isConnected ? 'connected' : 'disconnected'}`}></span>\r\n                {isConnected ? 'Connected' : 'Disconnected'}\r\n              </div>\r\n            </div>\r\n\r\n            <div className=\"messages-container\">\r\n              {messages.map((message, index) => (\r\n                <div key={index} className={`message-wrapper ${message.role === 'user' ? 'user-message' : 'bob-message'}`}>\r\n                  <div className=\"message-content\">\r\n                    <div className=\"message-sender\">\r\n                      {message.role === 'system' || message.role === 'assistant' ? 'Bob' : 'You'}\r\n                    </div>\r\n                    <div className=\"message-text\">{message.content}</div>\r\n                  </div>\r\n                  <div className=\"message-timestamp\">\r\n                    {new Date().toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })}\r\n                  </div>\r\n                </div>\r\n              ))}\r\n              <div ref={messagesEndRef} />\r\n            </div>\r\n\r\n            <div className=\"chat-controls\">\r\n              <button\r\n                className={`record-button ${isRecording ? 'recording' : ''} ${isProcessing ? 'processing' : ''} ${isInterviewComplete ? 'interview-complete' : ''}`}\r\n                onClick={toggleRecording}\r\n                disabled={isProcessing || isInterviewComplete}\r\n              >\r\n                <span className=\"button-icon\">\r\n                  {isRecording ? '⏹' : isProcessing ? '⌛' : isInterviewComplete ? '✅' : '🎤'}\r\n                </span>\r\n                <span className=\"button-text\">\r\n                  {isProcessing ? 'Processing...' : isRecording ? 'Stop Recording' : isInterviewComplete ? 'Interview Complete' : 'Start Recording'}\r\n                </span>\r\n              </button>\r\n              {isInterviewComplete && (\r\n                <button\r\n                  className=\"new-interview-button\"\r\n                  onClick={() => window.location.reload()}\r\n                >\r\n                  <span className=\"button-icon\">🔄</span>\r\n                  <span className=\"button-text\">Start New Interview</span>\r\n                </button>\r\n              )}\r\n            </div>\r\n          </div>\r\n\r\n          <RoomAudioRenderer />\r\n        </LiveKitRoom>\r\n      </div>\r\n    </>\r\n  );\r\n};\r\n\r\nexport default AudioChat; ","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\BackgroundAnimation.tsx",["88"],[],"import { useEffect, useRef } from 'react';\r\nimport * as THREE from 'three';\r\n\r\nconst BackgroundAnimation = () => {\r\n  const containerRef = useRef<HTMLDivElement>(null);\r\n  const rendererRef = useRef<THREE.WebGLRenderer | null>(null);\r\n  const sceneRef = useRef<THREE.Scene | null>(null);\r\n  const cameraRef = useRef<THREE.PerspectiveCamera | null>(null);\r\n  const cubesRef = useRef<THREE.Mesh[]>([]);\r\n  const velocitiesRef = useRef<{ x: number; y: number; z: number }[]>([]);\r\n  const timeRef = useRef<number>(0);\r\n  const boundingSpheresRef = useRef<THREE.Sphere[]>([]);\r\n\r\n  useEffect(() => {\r\n    if (!containerRef.current) return;\r\n\r\n    // Add maximum speed constant\r\n    const MAX_SPEED = 0.02; // Maximum speed for any direction\r\n\r\n    // Helper function to limit velocity\r\n    const limitVelocity = (velocity: { x: number; y: number; z: number }) => {\r\n      const speed = Math.sqrt(velocity.x * velocity.x + velocity.y * velocity.y + velocity.z * velocity.z);\r\n      if (speed > MAX_SPEED) {\r\n        const scale = MAX_SPEED / speed;\r\n        velocity.x *= scale;\r\n        velocity.y *= scale;\r\n        velocity.z *= scale;\r\n      }\r\n    };\r\n\r\n    // Setup\r\n    const container = containerRef.current;\r\n    const renderer = new THREE.WebGLRenderer({ \r\n      antialias: true,\r\n      alpha: true \r\n    });\r\n    renderer.setSize(container.clientWidth, container.clientHeight);\r\n    renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));\r\n    container.appendChild(renderer.domElement);\r\n    rendererRef.current = renderer;\r\n\r\n    // Camera\r\n    const fov = 75;\r\n    const aspect = container.clientWidth / container.clientHeight;\r\n    const near = 0.1;\r\n    const far = 1000;\r\n    const camera = new THREE.PerspectiveCamera(fov, aspect, near, far);\r\n    camera.position.z = 8;\r\n    cameraRef.current = camera;\r\n\r\n    // Scene\r\n    const scene = new THREE.Scene();\r\n    sceneRef.current = scene;\r\n\r\n    // Lighting\r\n    const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);\r\n    scene.add(ambientLight);\r\n\r\n    const directionalLight = new THREE.DirectionalLight(0xffffff, 1);\r\n    directionalLight.position.set(5, 5, 5);\r\n    scene.add(directionalLight);\r\n\r\n    // Add point lights for more subtle lighting\r\n    const pointLight1 = new THREE.PointLight(0x0072ff, 1.5, 20); // Reduced intensity, increased distance\r\n    pointLight1.position.set(-5, 3, 5);\r\n    scene.add(pointLight1);\r\n\r\n    const pointLight2 = new THREE.PointLight(0x8159ff, 1.5, 20); // Reduced intensity, increased distance\r\n    pointLight2.position.set(5, -3, 5);\r\n    scene.add(pointLight2);\r\n\r\n    // Create a subtle fog\r\n    scene.fog = new THREE.FogExp2(0x090d19, 0.03);\r\n\r\n    // Create cubes with modern materials\r\n    const geometryOptions = [\r\n      new THREE.BoxGeometry(1, 1, 1),\r\n      new THREE.IcosahedronGeometry(0.8, 0),\r\n      new THREE.OctahedronGeometry(0.8),\r\n      new THREE.TetrahedronGeometry(0.9)\r\n    ];\r\n\r\n    // Modern tech-inspired colors\r\n    const colors = [\r\n      0x0072ff, // Blue\r\n      0x8159ff, // Purple\r\n      0x2dd4bf, // Teal\r\n      0x0ea5e9, // Light blue\r\n      0x8b5cf6, // Violet\r\n      0x3b82f6, // Blue\r\n      0x0284c7, // Dark blue\r\n      0x2563eb, // Royal blue\r\n      0x7c3aed, // Purple\r\n      0x6366f1  // Indigo\r\n    ];\r\n\r\n    for (let i = 0; i < 15; i++) {\r\n      // Randomly select a geometry\r\n      const geometry = geometryOptions[Math.floor(Math.random() * geometryOptions.length)];\r\n      \r\n      // Create glassy material\r\n      const material = new THREE.MeshPhysicalMaterial({ \r\n        color: colors[i % colors.length],\r\n        transparent: true,\r\n        opacity: 0.6,\r\n        metalness: 0.2,\r\n        roughness: 0.25,\r\n        clearcoat: 0.5,\r\n        clearcoatRoughness: 0.2,\r\n        wireframe: Math.random() > 0.9, // Reduce wireframe probability\r\n      });\r\n      \r\n      const cube = new THREE.Mesh(geometry, material);\r\n      \r\n      // Random size\r\n      const scale = 0.5 + Math.random() * 0.7;\r\n      cube.scale.set(scale, scale, scale);\r\n      \r\n      // Random initial position (wider spread)\r\n      cube.position.x = (Math.random() - 0.5) * 16;\r\n      cube.position.y = (Math.random() - 0.5) * 16;\r\n      cube.position.z = (Math.random() - 0.5) * 16;\r\n      \r\n      // Create a bounding sphere for collision detection\r\n      // Size the bounding sphere based on the object's scale\r\n      const boundingSphere = new THREE.Sphere(\r\n        cube.position.clone(),\r\n        scale * 0.8 // Slightly smaller than the visual size for better visual collisions\r\n      );\r\n      boundingSpheresRef.current.push(boundingSphere);\r\n      \r\n      // Random initial velocity (much slower)\r\n      velocitiesRef.current.push({\r\n        x: (Math.random() - 0.5) * 0.006,\r\n        y: (Math.random() - 0.5) * 0.006,\r\n        z: (Math.random() - 0.5) * 0.006\r\n      });\r\n\r\n      scene.add(cube);\r\n      cubesRef.current.push(cube);\r\n    }\r\n\r\n    // Animation\r\n    const animate = (time: number) => {\r\n      timeRef.current = time * 0.001; // Convert to seconds\r\n      requestAnimationFrame(animate);\r\n\r\n      // Slowly rotate camera\r\n      const cameraRadius = 10;\r\n      const cameraSpeed = 0.02; // Reduced from 0.05 to 0.02\r\n      if (cameraRef.current) {\r\n        cameraRef.current.position.x = Math.sin(timeRef.current * cameraSpeed) * cameraRadius;\r\n        cameraRef.current.position.z = Math.cos(timeRef.current * cameraSpeed) * cameraRadius;\r\n        cameraRef.current.lookAt(0, 0, 0);\r\n      }\r\n\r\n      // First, update positions\r\n      cubesRef.current.forEach((cube, index) => {\r\n        const velocity = velocitiesRef.current[index];\r\n        \r\n        // Update position\r\n        cube.position.x += velocity.x;\r\n        cube.position.y += velocity.y;\r\n        cube.position.z += velocity.z;\r\n\r\n        // Update bounding sphere position\r\n        boundingSpheresRef.current[index].center.copy(cube.position);\r\n\r\n        // Bounce off boundaries\r\n        if (Math.abs(cube.position.x) > 8) velocity.x *= -1;\r\n        if (Math.abs(cube.position.y) > 8) velocity.y *= -1;\r\n        if (Math.abs(cube.position.z) > 8) velocity.z *= -1;\r\n\r\n        // Unique rotation for each cube\r\n        cube.rotation.x += 0.002 + index * 0.0001;\r\n        cube.rotation.y += 0.003 + index * 0.00005;\r\n        cube.rotation.z += 0.0025 + index * 0.0001;\r\n        \r\n        // Pulse scale effect based on sine wave\r\n        const pulseSpeed = 0.15 + index * 0.01;\r\n        const pulseStrength = 0.03;\r\n        const basescale = 0.5 + (index % 5) * 0.1;\r\n        const pulse = Math.sin(timeRef.current * pulseSpeed) * pulseStrength;\r\n        const newScale = basescale + pulse;\r\n        cube.scale.set(newScale, newScale, newScale);\r\n        \r\n        // Update bounding sphere radius with pulsing effect\r\n        boundingSpheresRef.current[index].radius = newScale * 0.8;\r\n      });\r\n      \r\n      // Check for collisions between all pairs of objects\r\n      for (let i = 0; i < cubesRef.current.length; i++) {\r\n        for (let j = i + 1; j < cubesRef.current.length; j++) {\r\n          const sphere1 = boundingSpheresRef.current[i];\r\n          const sphere2 = boundingSpheresRef.current[j];\r\n          \r\n          // Calculate distance between sphere centers\r\n          const distance = sphere1.center.distanceTo(sphere2.center);\r\n          const minDistance = sphere1.radius + sphere2.radius;\r\n          \r\n          // If spheres are colliding\r\n          if (distance < minDistance) {\r\n            // Get the objects and their velocities\r\n            const obj1 = cubesRef.current[i];\r\n            const obj2 = cubesRef.current[j];\r\n            const vel1 = velocitiesRef.current[i];\r\n            const vel2 = velocitiesRef.current[j];\r\n            \r\n            // Calculate collision normal\r\n            const normal = new THREE.Vector3()\r\n              .subVectors(sphere2.center, sphere1.center)\r\n              .normalize();\r\n            \r\n            // Apply impulse-based collision response\r\n            // Calculate relative velocity\r\n            const relativeVelocity = new THREE.Vector3(\r\n              vel2.x - vel1.x,\r\n              vel2.y - vel1.y,\r\n              vel2.z - vel1.z\r\n            );\r\n            \r\n            // Calculate impulse scale\r\n            const impulseScale = 1.5; // Can be adjusted for more \"bouncy\" collisions\r\n            const impulse = relativeVelocity.dot(normal) * impulseScale;\r\n            \r\n            // Apply impulse to velocities\r\n            vel1.x += normal.x * impulse;\r\n            vel1.y += normal.y * impulse;\r\n            vel1.z += normal.z * impulse;\r\n            \r\n            vel2.x -= normal.x * impulse;\r\n            vel2.y -= normal.y * impulse;\r\n            vel2.z -= normal.z * impulse;\r\n\r\n            // Limit velocities after collision\r\n            limitVelocity(vel1);\r\n            limitVelocity(vel2);\r\n            \r\n            // Move objects apart slightly to prevent sticking\r\n            const moveScale = (minDistance - distance) / 2;\r\n            obj1.position.x -= normal.x * moveScale;\r\n            obj1.position.y -= normal.y * moveScale;\r\n            obj1.position.z -= normal.z * moveScale;\r\n            \r\n            obj2.position.x += normal.x * moveScale;\r\n            obj2.position.y += normal.y * moveScale;\r\n            obj2.position.z += normal.z * moveScale;\r\n            \r\n            // Update bounding spheres after moving objects\r\n            boundingSpheresRef.current[i].center.copy(obj1.position);\r\n            boundingSpheresRef.current[j].center.copy(obj2.position);\r\n          }\r\n        }\r\n      }\r\n\r\n      renderer.render(scene, camera);\r\n    };\r\n\r\n    // Handle window resize\r\n    const handleResize = () => {\r\n      if (!containerRef.current || !cameraRef.current || !rendererRef.current) return;\r\n      \r\n      const container = containerRef.current;\r\n      const camera = cameraRef.current;\r\n      const renderer = rendererRef.current;\r\n\r\n      camera.aspect = container.clientWidth / container.clientHeight;\r\n      camera.updateProjectionMatrix();\r\n      renderer.setSize(container.clientWidth, container.clientHeight);\r\n      renderer.setPixelRatio(Math.min(window.devicePixelRatio, 2));\r\n    };\r\n\r\n    window.addEventListener('resize', handleResize);\r\n    animate(0);\r\n\r\n    // Cleanup\r\n    return () => {\r\n      window.removeEventListener('resize', handleResize);\r\n      if (containerRef.current && rendererRef.current) {\r\n        containerRef.current.removeChild(rendererRef.current.domElement);\r\n      }\r\n      cubesRef.current = [];\r\n      velocitiesRef.current = [];\r\n      boundingSpheresRef.current = [];\r\n    };\r\n  }, []);\r\n\r\n  return <div ref={containerRef} className=\"background-animation\" />;\r\n};\r\n\r\nexport default BackgroundAnimation; ","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\Chat.tsx",["89"],[],"'use client';\n\nimport { useEffect, useState } from 'react';\nimport { useChat } from 'ai/react';\nimport dynamic from 'next/dynamic';\nimport type { ChatBoxProps } from 'react-chat-plugin';\n\nconst ChatBox = dynamic<ChatBoxProps>(() => import('react-chat-plugin'), { ssr: false });\n\ntype ChatProps = {\n  initialText?: string;\n};\n\nconst userAuthor = {\n  username: 'User',\n  id: 1,\n  avatarUrl: 'https://cdn-icons-png.flaticon.com/512/149/149071.png',\n};\n\nconst aiAuthor = {\n  username: 'Bob The Interviewer',\n  id: 2,\n  avatarUrl: '/bob.jpg',\n};\n\nconst Chat: React.FC<ChatProps> = ({ initialText }) => {\n  const initialMessage = {\n    author: aiAuthor,\n    text: initialText ?? 'Hello, I am Bob the Interviewer. How can I help you?',\n    type: 'text',\n    timestamp: +new Date(),\n  };\n  const [chatMessages, setChatMessages] = useState([initialMessage]);\n  const { append, messages } = useChat({\n    api: '/api/openai-gpt',\n  });\n\n  useEffect(() => {\n    if (messages.length < 1) return;\n    const authors = {\n      user: userAuthor,\n      assistant: aiAuthor,\n    }\n    const chatMessagesArr = messages?.map(message => {\n      return ({\n        author: authors[message.role as keyof typeof authors],\n        text: message?.content,\n        type: 'text',\n        timestamp: +new Date(),\n      });\n    });\n    setChatMessages([initialMessage, ...chatMessagesArr]);\n  }, [messages]);\n\n  const handleOnSendMessage = (message: string) => {\n    append({\n      content: message,\n      role: 'user'\n    });\n  }\n\n  return (\n    <ChatBox\n      style={{margin: 'auto'}}\n      messages={chatMessages}\n      userId={1}\n      onSendMessage={handleOnSendMessage}\n      width={'550px'}\n      height={'500px'}\n    />\n  );\n}\n\nexport default Chat;","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\ResumeUploader.tsx",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\components\\SoundEffects.ts",["90"],[],"// SoundEffects.ts - Utility for playing sound effects in the application\r\n\r\nconst playSoundEffect = (soundUrl: string): void => {\r\n  try {\r\n    const audio = new Audio(soundUrl);\r\n    audio.play().catch(error => {\r\n      console.error('Error playing sound effect:', error);\r\n    });\r\n  } catch (error) {\r\n    console.error('Error initializing audio:', error);\r\n  }\r\n};\r\n\r\n// Sound effect utility functions\r\nexport const playTransformersSound = (): void => {\r\n  playSoundEffect('/transformers-sound.mp3');\r\n};\r\n\r\nexport const playGameEndSound = (): void => {\r\n  playSoundEffect('/game-end.mp3');\r\n};\r\n\r\nexport default {\r\n  playTransformersSound,\r\n  playGameEndSound\r\n}; ","C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\layout.tsx",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\page.tsx",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\pdfjs-dist-shims.d.ts",[],[],"C:\\Users\\JoseP\\Documents\\landingMarketingPages\\ip-clone-best-daily-test\\app\\react-chat-plugin.d.ts",[],[],{"ruleId":"91","severity":1,"message":"92","line":224,"column":6,"nodeType":"93","endLine":224,"endColumn":19,"suggestions":"94"},{"ruleId":"91","severity":1,"message":"92","line":280,"column":6,"nodeType":"93","endLine":280,"endColumn":19,"suggestions":"95"},{"ruleId":"91","severity":1,"message":"96","line":280,"column":22,"nodeType":"97","endLine":280,"endColumn":29},{"ruleId":"91","severity":1,"message":"98","line":53,"column":6,"nodeType":"93","endLine":53,"endColumn":16,"suggestions":"99"},{"ruleId":"100","severity":1,"message":"101","line":23,"column":1,"nodeType":"102","endLine":26,"endColumn":3},"react-hooks/exhaustive-deps","React Hook useEffect has a missing dependency: 'sendMessageToOpenAI'. Either include it or remove the dependency array.","ArrayExpression",["103"],["104"],"The ref value 'containerRef.current' will likely have changed by the time this effect cleanup function runs. If this ref points to a node rendered by React, copy 'containerRef.current' to a variable inside the effect, and use that variable in the cleanup function.","Identifier","React Hook useEffect has a missing dependency: 'initialMessage'. Either include it or remove the dependency array.",["105"],"import/no-anonymous-default-export","Assign object to a variable before exporting as module default","ExportDefaultDeclaration",{"desc":"106","fix":"107"},{"desc":"108","fix":"109"},{"desc":"110","fix":"111"},"Update the dependencies array to be: [initialText, sendMessageToOpenAI]",{"range":"112","text":"113"},"Update the dependencies array to be: [isConnected, sendMessageToOpenAI]",{"range":"114","text":"115"},"Update the dependencies array to be: [initialMessage, messages]",{"range":"116","text":"117"},[7282,7295],"[initialText, sendMessageToOpenAI]",[9177,9190],"[isConnected, sendMessageToOpenAI]",[1364,1374],"[initialMessage, messages]"]